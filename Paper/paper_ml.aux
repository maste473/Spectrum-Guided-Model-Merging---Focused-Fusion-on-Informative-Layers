\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\newlabel{tab:merging_strategies}{{1}{2}{Merging Strategies Logic}{table.1}{}}
\newlabel{fig:nlp_comparison}{{1}{3}{\textbf {NLP Task Analysis.} \textit {Top:} Performance on Emotion. The \textit {SNR Winner} strategy (green line) shows a progressive improvement, surpassing the \textit {Standard Merge} baseline (dashed line) as the retained spectrum increases (50\%-70\%). \textit {Bottom:} Performance on SST-2 (Sentiment). Here, the \textit {SNR Winner} consistently outperforms the baseline across all percentages, highlighting robustness in sentiment tasks, while \textit {Spectrum Base Reset} (blue line) requires higher retention rates to approach baseline performance}{figure.1}{}}
\newlabel{fig:nlp_cola_sst2}{{2}{3}{\textbf {NLP Task Analysis: CoLA vs. SST-2.} \textit {Top:} On CoLA (Grammar), the \textit {SNR Winner} strategy (orange bar) outperforms Standard Merge, proving that structural rules are localized in high-SNR weights. \textit {Bottom:} On SST-2 (Sentiment), spectral strategies fail to beat the Standard Merge baseline (blue bar), indicating that sentiment information is distributed and damaged by aggressive weight selection}{figure.2}{}}
\bibcite{marchenko1967}{{1}{}{{}}{{}}}
\bibcite{devlin2018}{{2}{}{{}}{{}}}
\bibcite{wortsman2022}{{3}{}{{}}{{}}}
\bibcite{hartford_spectrum}{{4}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\newlabel{fig:vision_vertical}{{3}{4}{\textbf {ResNet-18 Stability Analysis.} Top: Accuracy on Task C (FashionMNIST). Note how \texttt {snr\_lowB} (green line) causes a performance crash. Bottom: Accuracy on Task D (KMNIST), showing the opposite trend where \texttt {snr\_lowA} fails. This symmetry highlights the extreme sensitivity of convolutional layers to the "wrong" expert weights}{figure.3}{}}
\gdef \@abspage@last{4}
